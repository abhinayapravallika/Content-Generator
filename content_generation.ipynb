{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f9d5a7-4e4f-4c2e-8813-d3635b6f036e",
   "metadata": {},
   "source": [
    "# Story_Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d3b9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "\n",
      "**The Little Lion's Big Adventure**\n",
      "\n",
      "In the heart of the African savannah, a little lion named Leo lived with his best friend, a curious cat named Whiskers. Leo loved to roam the open plains, exploring every nook and cranny, while Whiskers preferred to lounge in the warm sun, chasing the occasional butterfly.\n",
      "\n",
      "One day, as they were lazing around, they stumbled upon a hidden path they had never seen before. The path was overgrown with tall grasses and colorful flowers, and it seemed to be calling their names. Leo, being the adventurous one, couldn't resist the urge to explore. \"Come on, Whiskers! Let's see where this path takes us!\" he exclaimed.\n",
      "\n",
      "Whiskers, always up for a challenge, followed close behind. As they walked, the path led them through a babbling brook, over a small hill, and into a dense thicket. Suddenly, they heard a faint meowing sound. \"What's that?\" Leo asked, his ears perked up.\n",
      "\n",
      "Whiskers sniffed the air, her whiskers twitching. \"It sounds like... kittens!\" she meowed. And indeed, they soon found themselves surrounded by a litter of playful kittens, tumbling and pouncing on each other.\n",
      "\n",
      "Leo and Whiskers watched in delight as the kittens chased each other around, their little paws waving in the air. But just as they were about to leave, one of the kittens tumbled into a nearby hole. \"Oh no! We have to help!\" Leo cried, and with Whiskers by his side, they carefully pulled the kitten out.\n",
      "\n",
      "The kittens' mother, a beautiful calico cat, appeared, grateful for their help. As a reward, she led them to a hidden clearing filled with juicy berries and sweet honey. Leo and Whiskers feasted on the treats, feeling like they'd discovered a treasure trove.\n",
      "\n",
      "As the sun began to set, they reluctantly said goodbye to their new friends and began their journey back home. \"That was the best day ever!\" Leo exclaimed, his tail wagging excitedly. Whiskers purred in agreement, already planning their next adventure.\n",
      "\n",
      "From that day on, Leo and Whiskers returned to the hidden path whenever they could, always discovering new wonders and making new friends along the way. And the little lion and his best friend, the curious cat, lived happily ever after, their bond stronger than ever.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "def get_context_type(query):\n",
    "    programming_languages = {\n",
    "    # Programming Languages\n",
    "    \"python\", \"java\", \"c\", \"c++\", \"c#\", \"javascript\", \"typescript\", \"go\", \"rust\", \"ruby\", \"perl\",\n",
    "    \"swift\", \"kotlin\", \"r\", \"dart\", \"php\", \"scala\", \"haskell\", \"lua\", \"matlab\", \"fortran\",\n",
    "    \"shell\", \"bash\", \"objective-c\", \"groovy\", \"lisp\", \"elixir\", \"clojure\", \"f#\", \"ada\", \"julia\",\n",
    "    \"cobol\", \"pascal\", \"abap\", \"prolog\", \"smalltalk\", \"scheme\", \"apl\", \"erlang\", \"modula-2\",\n",
    "    \"ocaml\", \"rexx\", \"tcl\", \"crystal\", \"forth\", \"hack\", \"idl\", \"icon\", \"j\", \"ml\", \"x10\", \"xquery\",\n",
    "    \"yaml\"\n",
    "\n",
    "    # Web Technologies\n",
    "    \"html\", \"css\", \"react\", \"angular\", \"vue\", \"node.js\", \"express.js\", \"django\", \"flask\",\n",
    "    \"spring boot\", \"next.js\", \"svelte\", \"tailwind\", \"bootstrap\", \"meteor.js\", \"nuxt.js\",\n",
    "    \"gatsby.js\", \"hugo\", \"jekyll\", \"blazor\", \"webassembly\", \"stencil.js\", \"alpine.js\"\n",
    "\n",
    "    # Databases\n",
    "    \"mysql\", \"postgresql\", \"mongodb\", \"sqlite\", \"oracle\", \"redis\", \"cassandra\", \"neo4j\",\n",
    "    \"sql server\", \"microsoft sql server\", \"firebase\", \"dynamodb\", \"cockroachdb\", \"mariadb\",\n",
    "    \"tarantool\", \"amazon aurora\", \"apache derby\", \"influxdb\", \"timescaledb\", \"clickhouse\",\n",
    "    \"tidb\", \"voltdb\",\n",
    "\n",
    "    # Cloud Platforms\n",
    "    \"aws\", \"azure\", \"gcp\", \"ibm cloud\", \"oracle cloud\", \"digitalocean\", \"linode\", \"heroku\",\n",
    "    \"netlify\", \"vercel\", \"cloudflare\",\n",
    "\n",
    "    # DevOps & CI/CD Tools\n",
    "    \"docker\", \"kubernetes\", \"jenkins\", \"terraform\", \"ansible\", \"prometheus\", \"grafana\",\n",
    "    \"git\", \"github\", \"gitlab\", \"helm\", \"openshift\", \"nomad\", \"puppet\", \"chef\", \"saltstack\",\n",
    "    \"argocd\", \"fluxcd\", \"spinnaker\", \"consul\",\n",
    "\n",
    "    # Operating Systems\n",
    "    \"windows\", \"linux\", \"macos\", \"android\", \"ios\", \"ubuntu\", \"redhat\", \"debian\", \"fedora\",\n",
    "    \"centos\", \"opensuse\", \"arch linux\", \"kali linux\", \"solaris\", \"aix\", \"freebsd\", \"haiku\",\n",
    "\n",
    "    # Networking Concepts\n",
    "    \"tcp/ip\", \"http\", \"https\", \"dns\", \"vpn\", \"firewall\", \"load balancer\", \"subnet\", \"proxy\",\n",
    "    \"dhcp\", \"bgp\", \"nat\", \"qos\", \"snmp\", \"ipsec\", \"mpls\", \"socks proxy\",\n",
    "\n",
    "    # Cybersecurity Topics\n",
    "    \"encryption\", \"firewalls\", \"penetration testing\", \"zero trust\", \"malware\", \"phishing\",\n",
    "    \"ransomware\", \"ethical hacking\", \"soc\", \"ids/ips\", \"siem\", \"cyber threat intelligence\",\n",
    "    \"dark web monitoring\",\n",
    "\n",
    "    # AI/ML Technologies\n",
    "    \"machine learning\", \"deep learning\", \"nlp\", \"computer vision\", \"tensorflow\", \"pytorch\",\n",
    "    \"scikit-learn\", \"keras\", \"fast.ai\", \"opencv\", \"deepmind\", \"lightgbm\", \"hugging face transformers\",\n",
    "    \"spacy\", \"gensim\", \"xgboost\", \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"power bi\", \"tableau\",\n",
    "    \"dask\", \"polars\", \"vaex\", \"altair\", \"plotly\", \"bokeh\", \"databricks\",\n",
    "\n",
    "    # Software Development Methodologies\n",
    "    \"agile\", \"scrum\", \"devops\", \"waterfall\", \"kanban\", \"extreme programming\", \"lean development\",\n",
    "    \"rad\", \"feature-driven development\",\n",
    "\n",
    "    # Blockchain Technologies\n",
    "    \"bitcoin\", \"ethereum\", \"smart contracts\", \"web3\", \"blockchain\", \"solana\", \"polkadot\",\n",
    "    \"hyperledger\", \"binance smart chain\", \"cosmos\", \"avalanche\", \"tezos\"\n",
    "\n",
    "    #full_stack_development\n",
    "    \"full stack\", \"mern stack\", \"mean stack\", \"lamp stack\", \"django full stack\", \"ruby on rails full stack\"\n",
    "    \n",
    "    #cse_subjects \n",
    "    \"data structures\", \"algorithms\", \"computer networks\", \"operating systems\", \"database management systems\",\n",
    "    \"software engineering\", \"artificial intelligence\", \"computer organization\", \"compiler design\",\n",
    "    \"web development\", \"mobile app development\", \"cryptography\", \"big data\", \"cloud computing\"\n",
    "    \n",
    "    \n",
    "}\n",
    "    return \"programming language\" if query.lower() in programming_languages else \"character\"\n",
    "\n",
    "def generate_response(topic, context, tone=None):\n",
    "    if context == \"character\":\n",
    "        template = \"\"\"\n",
    "        Write a {tone} story about {topic}.\n",
    "        The story should be engaging, creative, and follow a structured flow with an introduction,\n",
    "        middle, and end. Keep it under 450 words.\n",
    "        \"\"\"\n",
    "    elif context == \"programming language\":\n",
    "        template = \"\"\"\n",
    "        Provide an explanation of {topic}, covering its basics, use cases, and why it is important.\n",
    "        Include key features and examples. Keep the explanation concise and informative.\n",
    "        \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"topic\", \"tone\"] if context == \"character\" else [\"topic\"],\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    llm = ChatGroq(model_name=\"llama3-8b-8192\")\n",
    "    response = llm.invoke(prompt.format(topic=topic, tone=tone) if context == \"character\" else prompt.format(topic=topic))\n",
    "\n",
    "    return response.content if hasattr(response, \"content\") else str(response)\n",
    "\n",
    "topic = input(\"Enter a topic (e.g., name, programming language): \")\n",
    "context = get_context_type(topic)\n",
    "tone = input(\"Enter a tone (e.g., inspirational, humorous, suspenseful): \") if context == \"character\" else None\n",
    "\n",
    "response = generate_response(topic, context, tone)\n",
    "print(\"\\nGenerated Response:\\n\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30806b03-4f85-4813-89dc-9d5dcb7d2771",
   "metadata": {},
   "source": [
    "# Image Caption Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8124a51-9625-4a73-96a5-f4b71a872dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fece686-fa23-4b81-a319-0e010b85b0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vamsi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\vamsi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d91df199-3cdd-4dce-931b-05fc11e8dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "processor.save_pretrained(\"models/blip_model1\")\n",
    "model.save_pretrained(\"models/blip_model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10990265-1902-45c7-ad62-a161888caf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model load successfully\n"
     ]
    }
   ],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"models/blip_model1\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"models/blip_model2\")\n",
    "print(\"Model load successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9a6fd8-ea5b-4b98-92be-defe426ffca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_caption(image_path, processor, model):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(image, return_tensors=\"pt\")\n",
    "        output = model.generate(**inputs)\n",
    "        caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "    except Exception as e:\n",
    "        return f\"Error generating caption: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c3cef66-ef2d-4154-8402-639c8cfbf2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vamsi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: a circle with the words in red and orange\n"
     ]
    }
   ],
   "source": [
    "image_path = \"example.png\"\n",
    "caption = generate_image_caption(image_path, processor, model)\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "373ffc53-b9ef-48f3-bfa4-4d7cf49b02a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Captions:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Percentages Within Percentages Uncovered!',\n",
       " \"The Pie's the Limit!\",\n",
       " 'Nested Numbers Reveal All!',\n",
       " 'Breaking Down the Breakdown!',\n",
       " 'Layers of Insight Unfolding!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def generate_multiple_captions_with_llama(image_path, processor, model, llm, num_captions=5):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(image, return_tensors=\"pt\")\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            num_beams=num_captions,\n",
    "            num_return_sequences=num_captions,\n",
    "            max_length=50,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        blip_captions = [processor.decode(out, skip_special_tokens=True) for out in output]\n",
    "        refined_captions = []\n",
    "        for caption in blip_captions:\n",
    "            prompt = f\"\"\"Generate {num_captions} short, catchy, and creative captions (each under 8 words) for an image described as: '{caption}'. \n",
    "             Keep the captions crisp, engaging, and suitable for social media or titles. Avoid long sentences.\"\"\"\n",
    "\n",
    "        llm_response = llm.invoke(prompt)\n",
    "        c=llm_response.content\n",
    "        pattern = r'\\d+\\.\\s*\"(.*?)\"'\n",
    "        captions = re.findall(pattern,c)\n",
    "        return captions\n",
    "\n",
    "    except Exception as e:\n",
    "        return [f\"Error generating captions: {str(e)}\"]\n",
    "image_path = \"example.png\"\n",
    "captions = generate_multiple_captions_with_llama(image_path, processor, model, llm=ChatGroq(model_name=\"llama3-8b-8192\"), num_captions=5)\n",
    "print(\"Generated Captions:\\n\")\n",
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43016a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Captions:\n",
      "\n",
      "Global Vibes: Where People Live\n",
      "Pie-fectly Plotted Populations\n",
      "Country by Country, We Count\n",
      "Worldwide We're Connected, By Numbers\n",
      "Slice of Life: Global Population\n",
      "Piecing together the world's diversity\n",
      "Global Pie: Check out the breakdown\n",
      "Regions Revealed: A Slice of Data\n",
      "Where do we all fit in?\n",
      "The World in One Pie\n",
      "Pie-fectly Diagnosed: The Breakdown\n",
      "Stats that Slice the Truth\n",
      "The Reality of Diagnoses\n",
      "Graphic Breakdown: Who's Affected\n",
      "Charting the Truth: Diagnostic Stats\n",
      "Pie-fectly Precise: The Numbers Speak\n",
      "Percentages Galore: The Breakdown\n",
      "Unraveling the Pie: Inside Numbers\n",
      "Data Drama: The Pie in Action\n",
      "Slice of Truth: The Stats Revealed\n",
      "Percentages within Percentages: The Inside Scoop\n",
      "Unravel the Layers: Pie Chart Insights\n",
      "The Percent Within: Breakdown Revealed\n",
      "Drill Down: The Numbers Get Interesting\n",
      "The Inside Story: Percentages Exposed\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from groq import Client\n",
    "from PIL import Image\n",
    "\n",
    "def generate_multiple_captions_with_llama(image_path, processor, model, llm, num_captions=5):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(image, return_tensors=\"pt\")\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            num_beams=num_captions,\n",
    "            num_return_sequences=num_captions,\n",
    "            max_length=50,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        blip_captions = [processor.decode(out, skip_special_tokens=True) for out in output]\n",
    "        refined_captions = []\n",
    "\n",
    "        for caption in blip_captions:\n",
    "            prompt = f\"\"\"Generate {num_captions} short, catchy, and creative captions (each under 8 words) for an image described as: '{caption}'. \n",
    "             Keep the captions crisp, engaging, and suitable for social media or titles. Avoid long sentences.\"\"\"\n",
    "\n",
    "            llm_response = llm.chat.completions.create(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "\n",
    "            c = llm_response.choices[0].message.content  # Extract text from response\n",
    "            pattern = r'\\d+\\.\\s*\"(.*?)\"'\n",
    "            captions = re.findall(pattern, c)\n",
    "            refined_captions.extend(captions)\n",
    "\n",
    "        return refined_captions\n",
    "\n",
    "    except Exception as e:\n",
    "        return [f\"Error generating captions: {str(e)}\"]\n",
    "\n",
    "# Initialize Groq Client\n",
    "client = Client(api_key=\"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\")  # Replace with your actual API key\n",
    "\n",
    "# Run the function\n",
    "image_path = \"example.png\"\n",
    "captions = generate_multiple_captions_with_llama(image_path, processor, model, llm=client, num_captions=5)\n",
    "\n",
    "print(\"Generated Captions:\\n\")\n",
    "print(\"\\n\".join(captions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d159f1bd-d9b5-421c-8cfa-012723152d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Captions:\n",
      "\n",
      "Pie-fectly Global: Country Breakdown Revealed!\n",
      "The World in a Slice: By the Numbers\n",
      "Plot Twist: Where People Come From\n",
      "The Global Picture: A Step-by-Step\n",
      "How the World Lives: A Visual Guide\n",
      "By the numbers: regional breakdown\n",
      "Percentage perfect: regional insights\n",
      "Region by region: the stats\n",
      "Slicing up the data: by region\n",
      "Regional snapshot: percentages revealed\n",
      "Breaking it down, one slice at a time!\n",
      "The numbers don't lie, do they?\n",
      "Piecing together the truth\n",
      "The stats are in, what's yours?\n",
      "A slice above the rest\n",
      "Pie-fectly sliced numbers\n",
      "Percentages stacked high\n",
      "Visualizing the numbers game\n",
      "Slice and dice the stats\n",
      "Numbers in perfect harmony\n",
      "Pie-fectly Precise: Percentages Revealed!\n",
      "What's Inside? Percentage Breakdown!\n",
      "The Perfect Slice of Data\n",
      "Percentage Power: Unleashing Insights\n",
      "Statistically Sweet: Percentage Chart\n"
     ]
    }
   ],
   "source": [
    "# llm = ChatGroq(model_name=\"llama3-8b-8192\")\n",
    "# image_path = \"example.png\"\n",
    "# captions = generate_multiple_captions_with_llama(image_path, processor, model, llm, num_captions=5)\n",
    "\n",
    "# print(\"Generated Captions:\\n\")\n",
    "from groq import Client\n",
    "\n",
    "# Initialize Groq Client\n",
    "llm = Client(api_key=\"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\")  # Replace with your actual API key\n",
    "\n",
    "image_path = \"example.png\"\n",
    "captions = generate_multiple_captions_with_llama(image_path, processor, model, llm, num_captions=5)\n",
    "\n",
    "print(\"Generated Captions:\\n\")\n",
    "print(\"\\n\".join(captions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39f74c14-1bb5-4a67-9aa6-8b0cf9cacf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pie-fectly Global: Country Breakdown Revealed!',\n",
       " 'The World in a Slice: By the Numbers',\n",
       " 'Plot Twist: Where People Come From',\n",
       " 'The Global Picture: A Step-by-Step',\n",
       " 'How the World Lives: A Visual Guide',\n",
       " 'By the numbers: regional breakdown',\n",
       " 'Percentage perfect: regional insights',\n",
       " 'Region by region: the stats',\n",
       " 'Slicing up the data: by region',\n",
       " 'Regional snapshot: percentages revealed',\n",
       " 'Breaking it down, one slice at a time!',\n",
       " \"The numbers don't lie, do they?\",\n",
       " 'Piecing together the truth',\n",
       " \"The stats are in, what's yours?\",\n",
       " 'A slice above the rest',\n",
       " 'Pie-fectly sliced numbers',\n",
       " 'Percentages stacked high',\n",
       " 'Visualizing the numbers game',\n",
       " 'Slice and dice the stats',\n",
       " 'Numbers in perfect harmony',\n",
       " 'Pie-fectly Precise: Percentages Revealed!',\n",
       " \"What's Inside? Percentage Breakdown!\",\n",
       " 'The Perfect Slice of Data',\n",
       " 'Percentage Power: Unleashing Insights',\n",
       " 'Statistically Sweet: Percentage Chart']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e48945a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pie-fectly Global: Country Breakdown Revealed!\n",
      "The World in a Slice: By the Numbers\n",
      "Plot Twist: Where People Come From\n",
      "The Global Picture: A Step-by-Step\n",
      "How the World Lives: A Visual Guide\n",
      "By the numbers: regional breakdown\n",
      "Percentage perfect: regional insights\n",
      "Region by region: the stats\n",
      "Slicing up the data: by region\n",
      "Regional snapshot: percentages revealed\n",
      "Breaking it down, one slice at a time!\n",
      "The numbers don't lie, do they?\n",
      "Piecing together the truth\n",
      "The stats are in, what's yours?\n",
      "A slice above the rest\n",
      "Pie-fectly sliced numbers\n",
      "Percentages stacked high\n",
      "Visualizing the numbers game\n",
      "Slice and dice the stats\n",
      "Numbers in perfect harmony\n",
      "Pie-fectly Precise: Percentages Revealed!\n",
      "What's Inside? Percentage Breakdown!\n",
      "The Perfect Slice of Data\n",
      "Percentage Power: Unleashing Insights\n",
      "Statistically Sweet: Percentage Chart\n"
     ]
    }
   ],
   "source": [
    "for caption in captions:\n",
    "    print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23c1820e-d51e-450e-b6d0-3dbcfd6dbfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r'\\d+\\.\\s*\"(.*?)\"'\n",
    "# captions = re.findall(pattern,captions.content)\n",
    "# captions = re.findall(pattern, captions)\n",
    "captions_text = \"\\n\".join(captions)  \n",
    "captions = re.findall(pattern, captions_text)  \n",
    "\n",
    "    # Display the captions one by one\n",
    "for i, caption in enumerate(captions, start=1):\n",
    "    print(f\"{i}. {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a578bcdc-7e2d-4a70-9393-0bd6ac910a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d2e952-ecee-4732-aa44-1882cb1965e1",
   "metadata": {},
   "source": [
    "# Text To Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58b6212f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_groq in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2.4)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_groq) (0.18.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_groq) (0.3.40)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_groq) (0.3.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_groq) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_groq) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_groq) (6.0.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_groq) (24.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain_groq) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (3.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (1.26.18)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'DOSKEY' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "102e36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Directly set the API key (Not Secure)\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e050043d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The moon! Our closest celestial neighbor and a source of fascination for humans for centuries. Here are some interesting facts about the moon:\\n\\n1. **Distance from Earth**: The average distance from the Earth to the moon is about 384,400 kilometers (238,900 miles).\\n2. **Size**: The moon is the fifth-largest moon in the solar system, with a diameter of about 3,475 kilometers (2,160 miles).\\n3. **Orbital period**: It takes the moon about 27.3 days to complete one orbit around the Earth.\\n4. **Tidal locking**: The moon is tidally locked to the Earth, which means it always shows the same face to our planet.\\n5. **Phases**: The moon goes through eight distinct phases, which are determined by the amount of sunlight that reflects off its surface.\\n6. **Surface features**: The moon has many craters, mountains, and lava flows, which were formed by asteroid and comet impacts, volcanic activity, and other geological processes.\\n7. **Dark side**: The far side of the moon, sometimes called the \"dark side,\" was not visible to humans until the Soviet Union\\'s Luna 3 spacecraft imaged it in 1959.\\n8. **Moons of the moon**: The moon has no natural satellites of its own, but it does have several artificial satellites, including those launched by the United States, the Soviet Union, and China.\\n9. **Human exploration**: The United States and the Soviet Union both sent astronauts and cosmonauts to the moon in the late 1960s and early 1970s, with the last mission being Apollo 17 in December 1972.\\n10. **Future exploration**: NASA plans to return humans to the moon by 2024 under its Artemis program, with the goal of establishing a sustainable presence on the lunar surface.\\n\\nThese are just a few of the many fascinating facts about the moon. Is there something specific you\\'d like to know about our closest celestial neighbor?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 410, 'prompt_tokens': 11, 'total_tokens': 421, 'completion_time': 0.890406365, 'prompt_time': 0.006943294, 'queue_time': 0.262968857, 'total_time': 0.897349659}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_0fb809dba3', 'finish_reason': 'stop', 'logprobs': None} id='run-62a27aff-16ef-4f18-845e-b661502cb972-0' usage_metadata={'input_tokens': 11, 'output_tokens': 410, 'total_tokens': 421}\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(\"moon\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bd12875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The moon! Our closest celestial neighbor and a source of fascination for humans for centuries. Here are some interesting facts about the moon:\\n\\n1. **Distance from Earth**: The average distance from the Earth to the moon is about 384,400 kilometers (238,900 miles).\\n2. **Size**: The moon is the fifth-largest moon in the solar system, with a diameter of about 3,475 kilometers (2,160 miles).\\n3. **Orbital period**: It takes the moon about 27.3 days to complete one orbit around the Earth.\\n4. **Tidal locking**: The moon is tidally locked to the Earth, which means it always shows the same face to our planet.\\n5. **Phases**: The moon goes through eight distinct phases, which are determined by the amount of sunlight that reflects off its surface.\\n6. **Surface features**: The moon has many craters, mountains, and lava flows, which were formed by asteroid and comet impacts, volcanic activity, and other geological processes.\\n7. **Dark side**: The far side of the moon, sometimes called the \"dark side,\" was not visible to humans until the Soviet Union\\'s Luna 3 spacecraft imaged it in 1959.\\n8. **Moons of the moon**: The moon has no natural satellites of its own, but it does have several artificial satellites, including those launched by the United States, the Soviet Union, and China.\\n9. **Human exploration**: The United States and the Soviet Union both sent astronauts and cosmonauts to the moon in the late 1960s and early 1970s, with the last mission being Apollo 17 in December 1972.\\n10. **Future exploration**: NASA plans to return humans to the moon by 2024 under its Artemis program, with the goal of establishing a sustainable presence on the lunar surface.\\n\\nThese are just a few of the many fascinating facts about the moon. Is there something specific you\\'d like to know about our closest celestial neighbor?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "734aae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_required_points(text):\n",
    "    \"\"\"\n",
    "    Extracts only the main numbered points and the bullet points related to interesting facts.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text containing the full content.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of filtered points.\n",
    "    \"\"\"\n",
    "    # Extract main numbered points\n",
    "    numbered_points = re.findall(r'\\d+\\.\\s*\\*\\*(.*?)\\*\\*:?\\s*(.*?)(?=\\n\\d+\\.|\\Z)', text, re.S)\n",
    "    main_points = [f\"{idx + 1}. {title.strip()}: {desc.strip()}\" \n",
    "                   for idx, (title, desc) in enumerate(numbered_points)]\n",
    "\n",
    "    # Extract only bullet points from the \"Some other interesting facts\" section\n",
    "    bullet_section_match = re.search(r'Some other interesting facts about stars include:\\s*(.*?)\\Z', text, re.S)\n",
    "    bullet_points = []\n",
    "    if bullet_section_match:\n",
    "        bullet_text = bullet_section_match.group(1)\n",
    "        bullet_points = re.findall(r'\\*\\s*(.*?)\\n', bullet_text)\n",
    "\n",
    "    # Combine and return\n",
    "    extracted_points = main_points + bullet_points\n",
    "    return extracted_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "257a9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "input_text =  response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c12f9e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract points\n",
    "filtered_points = extract_required_points(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62b666c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Points:\n",
      "\n",
      "1. 1. Distance from Earth: The average distance from the Earth to the moon is about 384,400 kilometers (238,900 miles).\n",
      "2. 2. Size: The moon is the fifth-largest moon in the solar system, with a diameter of about 3,475 kilometers (2,160 miles).\n",
      "3. 3. Orbital period: It takes the moon about 27.3 days to complete one orbit around the Earth.\n",
      "4. 4. Tidal locking: The moon is tidally locked to the Earth, which means it always shows the same face to our planet.\n",
      "5. 5. Phases: The moon goes through eight distinct phases, which are determined by the amount of sunlight that reflects off its surface.\n",
      "6. 6. Surface features: The moon has many craters, mountains, and lava flows, which were formed by asteroid and comet impacts, volcanic activity, and other geological processes.\n",
      "7. 7. Dark side: The far side of the moon, sometimes called the \"dark side,\" was not visible to humans until the Soviet Union's Luna 3 spacecraft imaged it in 1959.\n",
      "8. 8. Moons of the moon: The moon has no natural satellites of its own, but it does have several artificial satellites, including those launched by the United States, the Soviet Union, and China.\n",
      "9. 9. Human exploration: The United States and the Soviet Union both sent astronauts and cosmonauts to the moon in the late 1960s and early 1970s, with the last mission being Apollo 17 in December 1972.\n",
      "10. 10. Future exploration: NASA plans to return humans to the moon by 2024 under its Artemis program, with the goal of establishing a sustainable presence on the lunar surface.\n",
      "\n",
      "These are just a few of the many fascinating facts about the moon. Is there something specific you'd like to know about our closest celestial neighbor?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display results\n",
    "print(\"Extracted Points:\\n\")\n",
    "for idx, point in enumerate(filtered_points, 1):\n",
    "    print(f\"{idx}. {point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9bf9f0-1c1b-421b-99b6-ce6c15d47385",
   "metadata": {},
   "source": [
    "# Video Content Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4db25e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: librosa in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: transformers in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.36.2)\n",
      "Requirement already satisfied: pytube in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (15.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pooch>=1.1->librosa) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\vamsi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'DOSKEY' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch librosa transformers pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6bd0d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Specify the model name\n",
    "model_name = \"openai/whisper-base\"\n",
    "\n",
    "# Load the processor and model\n",
    "l_processor = WhisperProcessor.from_pretrained(model_name)\n",
    "l_model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save the processor and model\n",
    "l_processor.save_pretrained(\"models/whisper_model_p\")\n",
    "l_model.save_pretrained(\"models/whisper_model_m\")\n",
    "\n",
    "# Confirmation message\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc533094",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'f:\\\\content_generation\\\\content_generation\\\\Group Discussion in English#english #shorts.m4a' -> '.\\\\a1.mp3'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileExistsError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m new_file = os.path.join(destination, \u001b[33m\"\u001b[39m\u001b[33ma1.mp3\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Rename file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Print confirmation message\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAudio downloaded and saved as: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileExistsError\u001b[39m: [WinError 183] Cannot create a file when that file already exists: 'f:\\\\content_generation\\\\content_generation\\\\Group Discussion in English#english #shorts.m4a' -> '.\\\\a1.mp3'"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "from pytubefix import YouTube\n",
    "import os\n",
    "\n",
    "# Get video URL from user\n",
    "yt = YouTube(input(\"Enter the URL of the video you want to download: \\n>> \"))\n",
    "\n",
    "# Extract only the audio\n",
    "video = yt.streams.filter(only_audio=True).first()\n",
    "\n",
    "# Set destination folder (default: current directory)\n",
    "destination = input(\"Enter the destination (leave blank for current directory): \\n>> \") or '.'\n",
    "\n",
    "# Download the audio file\n",
    "out_file = video.download(output_path=destination)\n",
    "\n",
    "# Save as a fixed name \"audio.mp3\"\n",
    "new_file = os.path.join(destination, \"a1.mp3\")\n",
    "\n",
    "# Rename file\n",
    "os.rename(out_file, new_file)\n",
    "\n",
    "# Print confirmation message\n",
    "print(f\"Audio downloaded and saved as: {new_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6987035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from pytube import YouTube\n",
    "\n",
    "class TranscriptGen:\n",
    "    def __init__(self, path, model_p=\"models/whisper_model_p\", model_m=\"models/whisper_model_m\", sampling_rate=16000, chunk_size=30):\n",
    "        self.path = path\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.chunk_size = chunk_size  # In seconds\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.processor = WhisperProcessor.from_pretrained(model_p)\n",
    "        self.model = WhisperForConditionalGeneration.from_pretrained(model_m).to(self.device)\n",
    "\n",
    "    def generate_transcript(self):\n",
    "        # Check if file exists before processing\n",
    "        if not os.path.exists(self.path):\n",
    "            raise FileNotFoundError(f\"Error: File '{self.path}' not found. Please check the filename and path.\")\n",
    "\n",
    "        # Load audio file\n",
    "        print(f\"Loading audio file: {self.path}\")\n",
    "        speech_array, sr = librosa.load(self.path, sr=self.sampling_rate)\n",
    "\n",
    "        chunk_samples = self.chunk_size * self.sampling_rate\n",
    "        total_samples = len(speech_array)\n",
    "        transcript = \"\"\n",
    "\n",
    "        for i in range(0, total_samples, chunk_samples):\n",
    "            chunk = speech_array[i: i + chunk_samples]\n",
    "            input_features = self.processor(chunk, sampling_rate=self.sampling_rate, return_tensors=\"pt\").input_features\n",
    "            input_features = input_features.to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                predicted_ids = self.model.generate(input_features)\n",
    "\n",
    "            chunk_transcription = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "            transcript += chunk_transcription + \" \"\n",
    "\n",
    "        return transcript.strip()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the correct file path\n",
    "    audio_path = os.path.join(os.getcwd(), \"a1.mp3\")  # Use absolute path\n",
    "\n",
    "    # Print file path for debugging\n",
    "    print(f\"Checking for file at: {audio_path}\")\n",
    "\n",
    "    transcriber = TranscriptGen(audio_path)\n",
    "    transcript = transcriber.generate_transcript()\n",
    "    print(\"\\n🔹 Transcription Output:\\n\", transcript)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
